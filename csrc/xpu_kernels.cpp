#include "xpu_kernels.h"
//#include "xpu_common.h"
#include <iostream>
#include <cmath>
#include <unistd.h>
#include <bit>

#include <sycl/sycl.hpp>

#if 0
static const float lookup_table[16] = {
    -1.0f,
    -0.6961928009986877f,
    -0.5250730514526367f,
    -0.39491748809814453f,
    -0.28444138169288635f,
    -0.18477343022823334f,
    -0.09105003625154495f,
    0.0f,
    0.07958029955625534f,
    0.16093020141124725f,
    0.24611230194568634f,
    0.33791524171829224f,
    0.44070982933044434f,
    0.5626170039176941f,
    0.7229568362236023f,
    1.0f};

template <typename T>
inline T dDequantizeNF4(uint8_t val) {
  return lookup_table[val]; // val < 16
}

template <
    typename T,
    int TILE_SIZE,
    int THREADS,
    int NUM_PER_TH,
    int DATA_TYPE>
SYCL_EXTERNAL void kDequantizeBlockwise_kernel(
    float* code,
    uint8_t* A,
    float* absmax,
    T* out,
    const int blocksize,
    const int n,
    sycl::nd_item<1>& item) {
  const int base_idx = (item.get_group(0) * TILE_SIZE);

  uint8_t qvals[NUM_PER_TH]; // quantized data
  T vals[NUM_PER_TH * 2]; // dequantized data

  float* qvals_f = reinterpret_cast<float*>(qvals);
  float* vals_f = reinterpret_cast<float*>(vals);

  float local_abs_max =
      absmax[(base_idx + item.get_local_id(0) * NUM_PER_TH) / (blocksize)];

  // load A to qvals
  float* A_f = reinterpret_cast<float*>(
      &A[(base_idx + item.get_local_id(0) * NUM_PER_TH)]);
#pragma unroll
  for (int j = 0; j < NUM_PER_TH / (sizeof(float) / sizeof(uint8_t)); j++) {
    qvals_f[j] = A_f[j];
  }

#pragma unroll
  for (int j = 0; j < NUM_PER_TH; j++) {
    // unpack to val and dequant
    vals[j * 2] =
        static_cast<T>(dDequantizeNF4<float>(qvals[j] >> 4) * local_abs_max);
    vals[j * 2 + 1] =
        static_cast<T>(dDequantizeNF4<float>(qvals[j] & 0x0F) * local_abs_max);
  }

  // write to output
  float* out_f = reinterpret_cast<float*>(
      &out[base_idx * 2 + item.get_local_id(0) * NUM_PER_TH * 2]);
#pragma unroll
  for (int j = 0; j < NUM_PER_TH * 2 / (sizeof(float) / sizeof(T)); j++) {
    out_f[j] = vals_f[j];
  }
}

template <
    typename T,
    int TILE_SIZE,
    int THREADS,
    int NUM_PER_TH,
    int DATA_TYPE>
SYCL_EXTERNAL void kDequantizeBlockwise<
    T,
    TILE_SIZE,
    THREADS,
    NUM_PER_TH,
    DATA_TYPE>::operator()(sycl::nd_item<1> item) const {
    kDequantizeBlockwise_kernel<
        T,
        TILE_SIZE,
        THREADS,
        NUM_PER_TH,
        DATA_TYPE>(code, A, absmax, out, blocksize, n, item);
  }

#endif 

#if 1
inline float dDequantizeFP4Tree(unsigned char val, float absmax)
{
  float sign = (val & 0b1000) == 8 ? -1.0f : 1.0f;
  if((val & 0b0100) == 4) // 0
    if((val & 0b0010) == 2) //01
      if((val & 0b0001) == 1) // 111
        return 0.25000000f*absmax*sign; // 1111
      else
        return 0.16666667f*absmax*sign; // 1110
    else
      if((val & 0b0001) == 1) // 110
        return 0.50000000f*absmax*sign; // 1101
      else
        return 0.33333333f*absmax*sign; // 1100
  else
    if((val & 0b0010) == 2) //10
      if((val & 0b0001) == 1) // 101
        return 1.00000000f*absmax*sign; // 1011
      else
        return 0.66666667f*absmax*sign; // 1010
    else
      if((val & 0b0001) == 1) // 100
        return 5.208333333e-03f*absmax*sign; // 1001
      else
        return 0.00000000f*absmax*sign; // 1000
}

inline float dDequantizeNF4(unsigned char val)
{

  // the values for this tree was generated by test_normal_map_tree
  // in the file tests/test_functional.py
  if((val & 0b1000) == 8)
    if((val & 0b0100) == 4) // 1
      if((val & 0b0010) == 2) // 11
        if((val & 0b0001) == 1) // 111
          return 1.0f; //*1111
        else
          return 0.7229568362236023f; //*1110
      else
        if((val & 0b0001) == 1) // 110
          return 0.5626170039176941f; //*1101
        else
          return 0.44070982933044434f; //*1100
    else
      if((val & 0b0010) == 2) //10
        if((val & 0b0001) == 1) // 101
          return 0.33791524171829224f; //*1011
        else
          return 0.24611230194568634f; //*1010
      else
        if((val & 0b0001) == 1) // 100
          return 0.16093020141124725f; //*1001
        else
          return 0.07958029955625534f; //*1000

  else
    if((val & 0b0100) == 4) // 0
      if((val & 0b0010) == 2) //01
        if((val & 0b0001) == 1) // 011
          return 0.0f; //*0111
        else
          return -0.09105003625154495f; //*0110
      else
        if((val & 0b0001) == 1) // 010
          return -0.18477343022823334f; //*0101
        else
          return -0.28444138169288635f; //*0100
    else
      if((val & 0b0010) == 2) //00
        if((val & 0b0001) == 1) // 001
          return -0.39491748809814453f; //*0011
        else
          return -0.5250730514526367f; //*0010
      else
        if((val & 0b0001) == 1) // 000
          return -0.6961928009986877f; //*0001
        else
          return -1.0f; //*0000

}

template <
    typename T,
    int TILE_SIZE,
    int THREADS,
    int NUM_PER_TH,
    int DATA_TYPE>
SYCL_EXTERNAL void kDequantizeBlockwise<
    T,
    TILE_SIZE,
    THREADS,
    NUM_PER_TH,
    DATA_TYPE>::operator()(sycl::nd_item<1> item) const {
//sycl::ext::oneapi::experimental::printf("this is kDequantizeBlockwise_kernel ...\n");
  const int base_idx = (item.get_group(0) * TILE_SIZE);
  size_t local_idx = item.get_local_id(0) * NUM_PER_TH;
  float local_abs_max = -FLT_MAX;
  const int n_load = (item.get_group_range(0) * TILE_SIZE);
  int valid_items_load = 0;
  int valid_items_store = 0;

  uint8_t qvals[NUM_PER_TH]; // quantized data
  T vals[NUM_PER_TH*((DATA_TYPE > 0) ? 2 : 1)]; // dequantized data
  //TODO: check whether for loop neccessary here.
  for (int i = base_idx; i < n_load; i += item.get_group_range(0) * TILE_SIZE) {
    //int i = base_idx;
    if (DATA_TYPE > 0) {
      valid_items_load = sycl::min(TILE_SIZE, (n + 1) / 2 - i);
      valid_items_store = sycl::min(TILE_SIZE * 2, n - i * 2);
    } else {
      valid_items_load = sycl::min(TILE_SIZE, n - i);
      valid_items_store = valid_items_load;
    }

sycl::ext::oneapi::experimental::printf("n_load = %d, n = %d, (n + 1) / 2 - i = %d, TILE_SIZE = %d, base_idx = %d, valid_items_load = %d, valid_items_store = %d\n",n_load, n,((n + 1) / 2 - i), TILE_SIZE, base_idx, valid_items_load, valid_items_store);
    // Avoid expensive divsion by the blocksize (as blocksize will always be a power-of-2)
    local_abs_max = absmax[(i + local_idx)  >> (31 - std::countl_zero<unsigned int>(blocksize))];
    sycl::ext::oneapi::experimental::printf("i = %d, item.get_local_id(0) = %d, NUM_PER_TH = %d, local_abs_max = %f\n",i,item.get_local_id(0),NUM_PER_TH,local_abs_max);
    sycl::ext::oneapi::experimental::printf("watch log .......\n");
    //reinterpret_cast<sycl::vec<unsigned char, 4>(&)[NUM_PER_TH]>(qvals)[0] = reinterpret_cast<sycl::vec<unsigned char, 4>*>(A)[i / NUM_PER_TH];
    item.barrier();
    auto local_src = &(A[i]);
    #pragma unroll //NUM_PER_TH
    for (int lt = 0; lt < NUM_PER_TH; lt++) {
      if (local_idx + lt < valid_items_load) {
        qvals[lt] = local_src[local_idx + lt];
      sycl::ext::oneapi::experimental::printf("blockload: item.get_local_id(0) = %d, local_src[%d] = %hhu, qvals[%d] = %hhu\n",item.get_local_id(0), (item.get_local_id(0) * NUM_PER_TH + lt), (uint8_t)local_src[item.get_local_id(0) * NUM_PER_TH + lt],lt,(uint8_t)qvals[lt]);
      } else {
        qvals[lt] = (unsigned char)0;
      sycl::ext::oneapi::experimental::printf("blockload default qvals[%d] = %b\n",lt,qvals[lt]);
      }
    }
    switch (DATA_TYPE)
    {
        case General8bit:
          // load code through read-only cache via __ldg
          //sycl::ext::oneapi::experimental::printf("this is General8bit ...\n");
          #pragma unroll //NUM_PER_TH
          for(int j = 0; j < NUM_PER_TH; j++)
            vals[j] = code[qvals[j]]*local_abs_max;
          break;
        case FP4:
        //TODO: check FP4 quant table with bitsandbytes/backends/utils.py, maybe not compitable.
        //  #pragma unroll NUM_PER_TH
        //  for(int j = 0; j < NUM_PER_TH; j++)
        //  {
        //    vals[j*2] = dDequantizeFP4Tree(qvals[j] >> 4, local_abs_max);
        //    vals[j*2 + 1] = dDequantizeFP4Tree(qvals[j] & 0x0F, local_abs_max);
        //  }
          break;
        case NF4:
          #pragma unroll //NUM_PER_TH
          for(int j = 0; j < NUM_PER_TH; j++)
          {
            vals[j*2] = dDequantizeNF4(qvals[j] >> 4)* local_abs_max;

            sycl::ext::oneapi::experimental::printf("qvals[%d] = %hhu, qvals[%d] >> 4 = %hhu, dDequantizeNF4(qvals[j] >> 4) = %f, local_abs_max = %f, vals[%d] = %f\n",j, (uint8_t)(qvals[j]), j, (uint8_t)(qvals[j] >> 4), dDequantizeNF4(qvals[j] >> 4), local_abs_max, j*2, vals[j*2]);

            vals[j*2 + 1] = dDequantizeNF4(qvals[j] & 0x0F)* local_abs_max;

            sycl::ext::oneapi::experimental::printf("qvals[%d] = %hhu, qvals[%d] & 0x0F = %hhu, dDequantizeNF4(qvals[j] & 0x0F) = %f, local_abs_max = %f, vals[%d] = %f\n",j,(uint8_t)(qvals[j]),j,(uint8_t) (qvals[j] & 0x0F), dDequantizeNF4(qvals[j] & 0x0F), local_abs_max, j*2+1, vals[j*2+1]);
          }
          break;
    }

    //reinterpret_cast<sycl::vec<float, 4>*>(out)[((DATA_TYPE > 0) ? i * 2 : i + local_idx * 2) / NUM_PER_TH * 2] = reinterpret_cast<sycl::vec<float, 4>*>(vals)[NUM_PER_TH * 2];
    item.barrier();
    auto local_dst = &(out[(DATA_TYPE > 0) ? i * 2 : i]);
    #pragma unroll //NUM_PER_TH
    for (int lt = 0; lt < NUM_PER_TH * 2 ; lt++) {
      if (lt < valid_items_store) {
        local_dst[local_idx * 2 + lt] = vals[lt];
        sycl::ext::oneapi::experimental::printf("blockstore: item.get_local_id(0) = %d, vals[%d] = %f, local_dst[%d] = %f\n",item.get_local_id(0), lt, (T)vals[lt], (item.get_local_id(0) * NUM_PER_TH * 2 + lt), local_dst[item.get_local_id(0) * NUM_PER_TH * 2 + lt]);
      }
    }
  }
}

#endif

#define num_values_4bit 32
template <typename T, int THREADS, int BITS, int SUBG_SIZE>
SYCL_EXTERNAL void kgemm_4bit_inference_kernel(int M, int N, int K, T * A, unsigned char *B,  float *absmax, const float *datatype, T * out,  int lda, int ldb, int ldc, int blocksize, sycl::local_ptr<T> quant_map, sycl::nd_item<1>& item) {
  size_t idx = item.get_local_id();
  const int sg_idx = idx / SUBG_SIZE;
  const int sg_lane = idx % SUBG_SIZE;
  const int row_B = (THREADS/SUBG_SIZE)*item.get_group().get_group_id() + sg_idx;
  const int offset_B = ldb * row_B;
  const int num_values_8bit = num_values_4bit / 2;
  float local_C = 0.0f;

  unsigned char local_B_4bit[num_values_8bit];
  T local_B[num_values_4bit/4];
  T local_A[num_values_4bit/4];
  T local_absmax = T(0.0f);

  if(idx < 16){
      quant_map[idx] = T(datatype[idx]);
  }

  item.barrier(sycl::access::fence_space::local_space);

  for(int inner_idx = sg_lane*num_values_4bit; inner_idx < K; inner_idx += SUBG_SIZE*num_values_4bit)
  {
    const int inner_idx_halved = inner_idx/2;

    // Avoid expensive divsion by the blocksize (as blocksize will always be a power-of-2)
    const int absidx = ((2*offset_B)+inner_idx) >> (31 - std::countl_zero((unsigned int)blocksize));
    local_absmax = absmax[absidx];

    if(row_B < M)
    {
      if((inner_idx_halved + num_values_8bit) < (K/2))
      {
        // this is the most important for performance considerations
        reinterpret_cast<sycl::vec<int, 4>(&)[num_values_8bit]>(local_B_4bit)[0] = reinterpret_cast<sycl::vec<int, 4>*>(B)[(offset_B+(inner_idx_halved))/(num_values_8bit)];
      }
      else
      {
        #pragma unroll
        for(int j = 0; j < (num_values_8bit); j++)
          if((inner_idx_halved) + j < (K/2))
            local_B_4bit[j] = B[offset_B+inner_idx_halved + j];
          else
            local_B_4bit[j] = 0b01110111;
      }
    }
    else
    {
      #pragma unroll
      for(int j = 0; j < (num_values_8bit); j++)
          local_B_4bit[j] = 0b01110111;
    }

    for(int i = 0; i < 4; i++)
    {
      #pragma unroll
      for(int k = 0; k < num_values_8bit/4; k++)
      {
        #if BNB_BF16_AVAILABLE
          local_B[k*2] = quant_map[local_B_4bit[(i*num_values_8bit/4) + k] >> 4]*local_absmax;
          local_B[k*2 + 1] = quant_map[local_B_4bit[(i*num_values_8bit/4) + k] & 0x0F]*local_absmax;
        #else
          // bf16 multipliation not supported
          local_B[k*2] = T((float)quant_map[local_B_4bit[(i*num_values_8bit/4) + k] >> 4]*(float)local_absmax);
          local_B[k*2 + 1] = T((float)quant_map[local_B_4bit[(i*num_values_8bit/4) + k] & 0x0F]*(float)local_absmax);
        #endif
      }

      if(inner_idx+(num_values_4bit/4) + (i*num_values_4bit/4) < K)
      {
        // this is also relatively important for performance
        if(BITS==16)
        {
          reinterpret_cast<sycl::vec<int, 4>(&)[num_values_4bit]>(local_A)[0] = reinterpret_cast<sycl::vec<int, 4>*>(A)[inner_idx/(num_values_4bit/4) + i];
        }
        else
        {
          reinterpret_cast<sycl::vec<int, 4>(&)[num_values_4bit]>(local_A)[0] = reinterpret_cast<sycl::vec<int, 4>*>(A)[inner_idx/(num_values_4bit/8) + (2*i) + 0];
          reinterpret_cast<sycl::vec<int, 4>(&)[num_values_4bit]>(local_A)[1] = reinterpret_cast<sycl::vec<int, 4>*>(A)[inner_idx/(num_values_4bit/8) + (2*i) + 1];
        }

      }
      else
        #pragma unroll
        for(int k = 0; k < num_values_4bit/4; k++)
          if(inner_idx + (i*num_values_4bit/4) + k < K)
            local_A[k] = A[inner_idx + k + (i*num_values_4bit/4)];
          else
            local_A[k] = T(0.0f);


      // accumulate in float; small performance hit for Ampere, but lower error for outputs
      #pragma unroll
      for(int k = 0; k < num_values_4bit/4; k++)
      {
          local_C += (float)(local_A[k]*local_B[k]);
      }
    }
  }

  local_C = sycl::reduce_over_group(item.get_sub_group(), local_C, sycl::plus<>());

  if(row_B < M && sg_lane == 0)
    out[row_B] = T(local_C);

}

template <typename T, int THREADS, int BITS, int SUBG_SIZE>
SYCL_EXTERNAL void kgemm_4bit_inference<T, THREADS, BITS, SUBG_SIZE>::operator()(sycl::nd_item<1> item) const {
    kgemm_4bit_inference_kernel<T, THREADS, BITS, SUBG_SIZE>(M, N, K, A, B, absmax, datatype, out, lda, ldb, ldc, blocksize, quant_map, item);
}
//==============================================================
//                   TEMPLATE DEFINITIONS
//==============================================================

template class kDequantizeBlockwise<sycl::half, 512, 128, 4, FP4>;
template class kDequantizeBlockwise<sycl::half, 512, 128, 4, General8bit>;
template class kDequantizeBlockwise<sycl::half, 512, 128, 4, NF4>;

template class kDequantizeBlockwise<float, 512, 128, 4, FP4>;
template class kDequantizeBlockwise<float, 512, 128, 4, General8bit>;
template class kDequantizeBlockwise<float, 512, 128, 4, NF4>;

template class kDequantizeBlockwise<sycl::ext::oneapi::bfloat16, 512, 128, 4, FP4>;
template class kDequantizeBlockwise<sycl::ext::oneapi::bfloat16, 512, 128, 4, General8bit>;
template class kDequantizeBlockwise<sycl::ext::oneapi::bfloat16, 512, 128, 4, NF4>;

template class kgemm_4bit_inference<sycl::half, 128, 16, 32>;
template class kgemm_4bit_inference<sycl::ext::oneapi::bfloat16, 128, 16, 32>;
template class kgemm_4bit_inference<float, 128, 32, 32>;
